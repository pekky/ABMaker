# -*- coding: utf-8 -*-
"""
è¶…å†…å­˜ä¼˜åŒ–ç‰ˆæœ‰å£°è¯»ç‰©åˆ¶ä½œå™¨
å……åˆ†åˆ©ç”¨GPUæ˜¾å­˜åˆ°70%ï¼Œæœ€å¤§åŒ–å¤„ç†é€Ÿåº¦
"""

import os
import sys
import time
import json
import argparse
from datetime import datetime
from typing import List, Dict, Any
import numpy as np
from scipy.io import wavfile
import torch
import concurrent.futures
import gc
import threading
import queue

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from smart_pdf_extractor import SmartPDFExtractor
from core.text_processor import TextProcessor

# ä¿®å¤torch.loadå…¼å®¹æ€§
import pickle
original_load = torch.load
def patched_load(f, map_location=None, pickle_module=pickle, **kwargs):
    return original_load(f, map_location=map_location, pickle_module=pickle, **kwargs)
torch.load = patched_load

class UltraMemoryOptimizedAudioGenerator:
    """è¶…å†…å­˜ä¼˜åŒ–ç‰ˆéŸ³é¢‘ç”Ÿæˆå™¨ï¼Œå……åˆ†åˆ©ç”¨GPUæ˜¾å­˜åˆ°70%"""
    
    def __init__(self, voice_preset: str = "v2/zh_speaker_1", 
                 max_workers: int = 16,  # å¤§å¹…å¢åŠ å¹¶è¡Œæ•°
                 batch_size: int = 32,   # å¤§å¹…å¢åŠ æ‰¹å¤„ç†å¤§å°
                 memory_target_percent: int = 70):  # ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡
        self.voice_preset = voice_preset
        self.sample_rate = 24000
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.memory_target_percent = memory_target_percent
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æœ€å¤§åŒ–æ˜¾å­˜ä½¿ç”¨
        os.environ["SUNO_USE_SMALL_MODELS"] = "False"
        os.environ["SUNO_OFFLOAD_CPU"] = "False"
        
        print("ğŸš€ è¶…å†…å­˜ä¼˜åŒ–æ¨¡å¼å¯åŠ¨")
        print(f"âœ“ ä½¿ç”¨å®Œæ•´æ¨¡å‹ï¼ˆæœ€é«˜è´¨é‡ï¼‰")
        print(f"âœ“ ç¦ç”¨CPUå¸è½½ï¼ˆæœ€å¤§åŒ–GPUæ˜¾å­˜ä½¿ç”¨ï¼‰")
        print(f"âœ“ å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {max_workers}")
        print(f"âœ“ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"âœ“ ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡: {memory_target_percent}%")
        
        # æ£€æŸ¥GPUæ˜¾å­˜
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            target_memory = gpu_memory * memory_target_percent / 100
            print(f"âœ“ GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
            print(f"âœ“ ç›®æ ‡ä½¿ç”¨æ˜¾å­˜: {target_memory:.1f}GB")
            
            # è®¾ç½®CUDAä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.allow_tf32 = False
            print("âœ“ å¯ç”¨CUDAä¼˜åŒ–")
        
        # é¢„åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½Barkæ¨¡å‹...")
        from bark import SAMPLE_RATE, generate_audio, preload_models
        self.sample_rate = SAMPLE_RATE
        preload_models()
        print("âœ“ Barkæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # é¢„çƒ­GPUåˆ°ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡
        self._warmup_gpu_to_target()
        
        # åˆ›å»ºå¤„ç†é˜Ÿåˆ—
        self.audio_queue = queue.Queue(maxsize=max_workers * 2)
        self.result_queue = queue.Queue()
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self.workers = []
        self._start_workers()
    
    def _warmup_gpu_to_target(self):
        """é¢„çƒ­GPUåˆ°ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡"""
        if not torch.cuda.is_available():
            return
            
        print(f"æ­£åœ¨é¢„çƒ­GPUåˆ°{self.memory_target_percent}%æ˜¾å­˜ä½¿ç”¨ç‡...")
        
        # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨
        current_memory = torch.cuda.memory_allocated() / 1024**3
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        target_memory = total_memory * self.memory_target_percent / 100
        
        print(f"  å½“å‰æ˜¾å­˜ä½¿ç”¨: {current_memory:.1f}GB")
        print(f"  ç›®æ ‡æ˜¾å­˜ä½¿ç”¨: {target_memory:.1f}GB")
        
        # åˆ›å»ºå¼ é‡æ¥å ç”¨æ˜¾å­˜
        dummy_tensors = []
        try:
            while current_memory < target_memory * 0.8:  # ç•™ä¸€äº›ä½™é‡
                # åˆ›å»ºå¤§å¼ é‡
                tensor_size = min(1024, int((target_memory - current_memory) * 1024 * 0.1))
                if tensor_size < 100:
                    break
                    
                tensor = torch.randn(tensor_size, tensor_size, device='cuda')
                dummy_tensors.append(tensor)
                
                current_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"  é¢„çƒ­è¿›åº¦: {current_memory:.1f}GB / {target_memory:.1f}GB")
                
        except RuntimeError as e:
            print(f"  é¢„çƒ­å®Œæˆ: {e}")
        
        print(f"âœ“ GPUé¢„çƒ­å®Œæˆï¼Œå½“å‰æ˜¾å­˜ä½¿ç”¨: {current_memory:.1f}GB")
    
    def _start_workers(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        print(f"å¯åŠ¨ {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")
        
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._worker_loop, daemon=True)
            worker.start()
            self.workers.append(worker)
        
        print("âœ“ å·¥ä½œçº¿ç¨‹å¯åŠ¨å®Œæˆ")
    
    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹å¾ªç¯"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.audio_queue.get(timeout=1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                
                text, task_id = task
                audio_array = self._generate_single_audio(text)
                
                # å°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—
                self.result_queue.put((task_id, audio_array))
                self.audio_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                self.audio_queue.task_done()
    
    def _generate_single_audio(self, text: str) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªéŸ³é¢‘ç‰‡æ®µ"""
        try:
            from bark import generate_audio
            audio_array = generate_audio(text, history_prompt=self.voice_preset)
            return audio_array.astype(np.float32)
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return np.zeros(int(self.sample_rate * 0.5), dtype=np.float32)
    
    def generate_audio_batch(self, text_chunks: List[str]) -> List[np.ndarray]:
        """æ‰¹é‡ç”ŸæˆéŸ³é¢‘ï¼Œæœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨"""
        print(f"ğŸµ å¼€å§‹è¶…æ‰¹é‡ç”Ÿæˆ {len(text_chunks)} ä¸ªéŸ³é¢‘ç‰‡æ®µ...")
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡åˆ°é˜Ÿåˆ—
        for i, chunk in enumerate(text_chunks):
            self.audio_queue.put((chunk, i))
        
        # æ”¶é›†ç»“æœ
        audio_arrays = [None] * len(text_chunks)
        completed = 0
        
        while completed < len(text_chunks):
            try:
                task_id, audio_array = self.result_queue.get(timeout=30)
                audio_arrays[task_id] = audio_array
                completed += 1
                
                if completed % 50 == 0 or completed == len(text_chunks):
                    progress = completed / len(text_chunks) * 100
                    print(f"  è¿›åº¦: {completed}/{len(text_chunks)} ({progress:.1f}%)")
                    
                    # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                    if torch.cuda.is_available():
                        current_memory = torch.cuda.memory_allocated() / 1024**3
                        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                        usage_percent = current_memory / total_memory * 100
                        print(f"  GPUæ˜¾å­˜: {current_memory:.1f}GB/{total_memory:.1f}GB ({usage_percent:.1f}%)")
                
            except queue.Empty:
                print("âš ï¸ ç­‰å¾…ç»“æœè¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…...")
                continue
        
        print(f"âœ“ è¶…æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(audio_arrays)} ä¸ªéŸ³é¢‘ç‰‡æ®µ")
        return audio_arrays
    
    def merge_audio_arrays(self, audio_arrays: List[np.ndarray], output_path: str, 
                          silence_duration: float = 0.05) -> str:  # å‡å°‘é™éŸ³æ—¶é—´
        """åˆå¹¶éŸ³é¢‘æ•°ç»„"""
        if not audio_arrays:
            return None
        
        print(f"ğŸ”— æ­£åœ¨åˆå¹¶ {len(audio_arrays)} ä¸ªéŸ³é¢‘ç‰‡æ®µ...")
        
        # è®¡ç®—æ€»é•¿åº¦
        total_length = sum(len(audio) for audio in audio_arrays)
        silence_samples = int(self.sample_rate * silence_duration)
        total_length += silence_samples * (len(audio_arrays) - 1)
        
        # åˆ›å»ºåˆå¹¶åçš„éŸ³é¢‘æ•°ç»„
        merged_audio = np.zeros(total_length, dtype=np.float32)
        
        current_pos = 0
        for i, audio in enumerate(audio_arrays):
            merged_audio[current_pos:current_pos + len(audio)] = audio
            current_pos += len(audio)
            
            # æ·»åŠ é™éŸ³é—´éš”ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(audio_arrays) - 1:
                current_pos += silence_samples
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        wavfile.write(output_path, self.sample_rate, merged_audio)
        print(f"âœ“ éŸ³é¢‘å·²ä¿å­˜: {output_path}")
        
        return output_path

class UltraMemoryOptimizedAudiobookMaker:
    """è¶…å†…å­˜ä¼˜åŒ–ç‰ˆæœ‰å£°è¯»ç‰©åˆ¶ä½œå™¨"""
    
    def __init__(self, voice_preset: str = "v2/zh_speaker_1",
                 max_chars_per_chunk: int = 150,  # å‡å°‘ç‰‡æ®µå¤§å°ä»¥å¢åŠ å¹¶è¡Œåº¦
                 target_tokens_per_batch: int = 50000,  # å¢åŠ æ‰¹æ¬¡å¤§å°
                 max_workers: int = 16,
                 batch_size: int = 32,
                 memory_target_percent: int = 70,
                 resume: bool = True):
        self.voice_preset = voice_preset
        self.max_chars_per_chunk = max_chars_per_chunk
        self.target_tokens_per_batch = target_tokens_per_batch
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.memory_target_percent = memory_target_percent
        self.resume = resume
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_processor = TextProcessor(max_chars=max_chars_per_chunk)
        self.audio_generator = UltraMemoryOptimizedAudioGenerator(
            voice_preset=voice_preset,
            max_workers=max_workers,
            batch_size=batch_size,
            memory_target_percent=memory_target_percent
        )
        
        # çŠ¶æ€æ–‡ä»¶
        self.state_file = "ultra_memory_optimized_processing_state.json"
    
    def save_state(self, state: Dict[str, Any]):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    
    def load_state(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def create_audiobook_batch(self, pdf_path: str, output_dir: str, 
                              voice_preset: str, max_chars_per_chunk: int, 
                              target_tokens_per_batch: int, keep_chunks: bool) -> str:
        """åˆ›å»ºæœ‰å£°è¯»ç‰©æ‰¹æ¬¡"""
        
        print("ğŸš€ å¼€å§‹è¶…å†…å­˜ä¼˜åŒ–ç‰ˆæœ‰å£°è¯»ç‰©åˆ¶ä½œ")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æˆ–åˆ›å»ºçŠ¶æ€
        state = self.load_state() if self.resume else {}
        
        # æ­¥éª¤1: æå–PDFæ–‡æœ¬å¹¶åˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡
        if 'smart_batches' not in state:
            print("\næ­¥éª¤ 1/3: æ™ºèƒ½æå–PDFæ–‡æœ¬")
            print("-" * 40)
            
            extractor = SmartPDFExtractor(pdf_path)
            structure_data = extractor.extract_text_with_structure()
            
            # åˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡
            smart_batches = self._create_smart_batches(structure_data, target_tokens_per_batch)
            
            state.update({
                'pdf_path': pdf_path,
                'output_dir': output_dir,
                'voice_preset': voice_preset,
                'max_chars_per_chunk': max_chars_per_chunk,
                'target_tokens_per_batch': target_tokens_per_batch,
                'smart_batches': smart_batches,
                'total_batches': len(smart_batches),
                'completed_batches': [],
                'start_time': datetime.now().isoformat(),
                'last_update': datetime.now().isoformat()
            })
            
            self.save_state(state)
            
            print(f"âœ“ PDFæ™ºèƒ½æå–å®Œæˆ")
            print(f"âœ“ æ€»é¡µæ•°: {len(structure_data['pages'])}")
            print(f"âœ“ æ€»å­—ç¬¦æ•°: {sum(len(page['text']) for page in structure_data['pages']):,}")
            print(f"âœ“ æ™ºèƒ½æ‰¹æ¬¡: {len(smart_batches)} æ‰¹")
        
        # æ­¥éª¤2: å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        print(f"\næ­¥éª¤ 2/3: è¶…å†…å­˜ä¼˜åŒ–éŸ³é¢‘ç”Ÿæˆ")
        print("-" * 40)
        
        smart_batches = state['smart_batches']
        total_batches = state['total_batches']
        completed_batches = state.get('completed_batches', [])
        
        print(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        print(f"å·²å®Œæˆ: {len(completed_batches)} æ‰¹")
        print(f"å‰©ä½™: {total_batches - len(completed_batches)} æ‰¹")
        
        for batch_idx, batch_info in enumerate(smart_batches):
            if batch_idx in completed_batches:
                print(f"â­ï¸  è·³è¿‡å·²å®Œæˆçš„æ‰¹æ¬¡ {batch_idx + 1}")
                continue
            
            print(f"\n--- å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ---")
            print(f"é¡µé¢èŒƒå›´: ç¬¬{batch_info['start_page']}-{batch_info['end_page']}é¡µ")
            print(f"Tokenæ•°: {batch_info['tokens']:,}")
            print(f"å­—ç¬¦æ•°: {batch_info['chars']:,}")
            
            # åˆ†å‰²æ–‡æœ¬ä¸ºç‰‡æ®µ
            batch_chunks = self.text_processor.split_into_chunks(batch_info['content'])
            print(f"âœ“ æ–‡æœ¬å·²åˆ†å‰²æˆ {len(batch_chunks)} ä¸ªç‰‡æ®µ")
            
            # ç”ŸæˆéŸ³é¢‘
            batch_start_time = time.time()
            audio_arrays = self.audio_generator.generate_audio_batch(batch_chunks)
            batch_time = time.time() - batch_start_time
            
            print(f"âœ“ éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {batch_time/60:.1f} åˆ†é’Ÿ")
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            output_path = os.path.join(output_dir, f"batch_{batch_idx + 1:03d}.wav")
            self.audio_generator.merge_audio_arrays(audio_arrays, output_path)
            
            # ä¿å­˜ç‰‡æ®µæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if keep_chunks:
                chunk_dir = os.path.join(output_dir, f"batch_{batch_idx + 1:03d}_chunks")
                os.makedirs(chunk_dir, exist_ok=True)
                
                for i, (chunk, audio) in enumerate(zip(batch_chunks, audio_arrays)):
                    chunk_path = os.path.join(chunk_dir, f"chunk_{i:04d}.wav")
                    wavfile.write(chunk_path, self.audio_generator.sample_rate, audio)
            
            # æ›´æ–°çŠ¶æ€
            completed_batches.append(batch_idx)
            state['completed_batches'] = completed_batches
            state['last_update'] = datetime.now().isoformat()
            self.save_state(state)
            
            print(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_idx < total_batches - 1:
                print("æ‰¹æ¬¡é—´ä¼‘æ¯ 2 ç§’...")
                time.sleep(2)
        
        # æ­¥éª¤3: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨
        print(f"\næ­¥éª¤ 3/3: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
        print("-" * 40)
        
        playlist_path = os.path.join(output_dir, "playlist.m3u")
        with open(playlist_path, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for i in range(total_batches):
                f.write(f"#EXTINF:-1,æ‰¹æ¬¡ {i + 1}\n")
                f.write(f"batch_{i + 1:03d}.wav\n")
        
        print(f"âœ“ æ’­æ”¾åˆ—è¡¨å·²ç”Ÿæˆ: {playlist_path}")
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = datetime.now() - datetime.fromisoformat(state['start_time'])
        print(f"\nğŸ‰ è¶…å†…å­˜ä¼˜åŒ–ç‰ˆå¤„ç†å®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        return output_dir
    
    def _create_smart_batches(self, structure_data: Dict, target_tokens: int) -> List[Dict]:
        """åˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡"""
        batches = []
        current_batch_pages = []
        current_batch_chars = 0
        current_batch_paragraphs = []
        
        pages = structure_data['pages']
        
        for page_idx, page in enumerate(pages):
            page_text = page['text']
            page_paragraphs = page.get('paragraphs', [page_text])
            
            # å°è¯•æ·»åŠ æ•´ä¸ªé¡µé¢
            if current_batch_chars + len(page_text) <= target_tokens * 3 or not current_batch_pages:
                current_batch_pages.append(page['page_number'])
                current_batch_chars += len(page_text)
                current_batch_paragraphs.extend(page_paragraphs)
            else:
                # å®Œæˆå½“å‰æ‰¹æ¬¡
                if current_batch_pages:
                    batches.append({
                        'start_page': current_batch_pages[0],
                        'end_page': current_batch_pages[-1],
                        'chars': current_batch_chars,
                        'tokens': current_batch_chars // 4,
                        'content': '\n\n'.join(current_batch_paragraphs)
                    })
                
                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch_pages = [page['page_number']]
                current_batch_chars = len(page_text)
                current_batch_paragraphs = page_paragraphs
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch_pages:
            batches.append({
                'start_page': current_batch_pages[0],
                'end_page': current_batch_pages[-1],
                'chars': current_batch_chars,
                'tokens': current_batch_chars // 4,
                'content': '\n\n'.join(current_batch_paragraphs)
            })
        
        return batches

def main():
    parser = argparse.ArgumentParser(description="è¶…å†…å­˜ä¼˜åŒ–ç‰ˆPDFè½¬æœ‰å£°è¯»ç‰©")
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output_dir", default="ultra_memory_optimized_audio", help="è¾“å‡ºç›®å½•")
    parser.add_argument("-v", "--voice_preset", default="v2/en_speaker_6", help="è¯­éŸ³é¢„è®¾")
    parser.add_argument("-c", "--max_chars_per_chunk", type=int, default=150, help="æ¯ç‰‡æ®µæœ€å¤§å­—ç¬¦æ•°")
    parser.add_argument("-t", "--target_tokens_per_batch", type=int, default=50000, help="ç›®æ ‡æ‰¹æ¬¡Tokenæ•°")
    parser.add_argument("-w", "--max_workers", type=int, default=16, help="æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("-b", "--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("-m", "--memory_target_percent", type=int, default=70, help="ç›®æ ‡æ˜¾å­˜ä½¿ç”¨ç‡")
    parser.add_argument("--keep-chunks", action="store_true", help="ä¿ç•™éŸ³é¢‘ç‰‡æ®µæ–‡ä»¶")
    parser.add_argument("--no-resume", action="store_true", help="ä¸æ¢å¤ä¹‹å‰çš„å¤„ç†")
    
    args = parser.parse_args()
    
    maker = UltraMemoryOptimizedAudiobookMaker(
        voice_preset=args.voice_preset,
        max_chars_per_chunk=args.max_chars_per_chunk,
        target_tokens_per_batch=args.target_tokens_per_batch,
        max_workers=args.max_workers,
        batch_size=args.batch_size,
        memory_target_percent=args.memory_target_percent,
        resume=not args.no_resume
    )
    
    maker.create_audiobook_batch(
        pdf_path=args.pdf_path,
        output_dir=args.output_dir,
        voice_preset=args.voice_preset,
        max_chars_per_chunk=args.max_chars_per_chunk,
        target_tokens_per_batch=args.target_tokens_per_batch,
        keep_chunks=args.keep_chunks
    )

if __name__ == "__main__":
    main()
