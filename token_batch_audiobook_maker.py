# -*- coding: utf-8 -*-
"""
æŒ‰4ä¸‡tokenåˆ†æ‰¹å¤„ç†æœ‰å£°è¯»ç‰©åˆ¶ä½œå™¨ - æ¯æ‰¹ç”Ÿæˆä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
"""
import os
import argparse
import json
import time
from datetime import datetime
from typing import List
from smart_pdf_extractor import SmartPDFExtractor
from text_processor import TextProcessor
from efficient_audio_generator import EfficientAudioGenerator


class TokenBatchAudiobookMaker:
    """æŒ‰4ä¸‡tokenåˆ†æ‰¹å¤„ç†æœ‰å£°è¯»ç‰©åˆ¶ä½œå™¨"""
    
    def __init__(self, voice_preset: str = "v2/zh_speaker_1", 
                 max_chars: int = 200,  # ç‰‡æ®µå¤§å°
                 target_tokens: int = 40000,  # ç›®æ ‡tokenæ•°ï¼ˆ4ä¸‡ï¼‰
                 resume: bool = True,
                 max_workers: int = 6):
        """
        åˆå§‹åŒ–æŒ‰tokenåˆ†æ‰¹å¤„ç†åˆ¶ä½œå™¨
        
        Args:
            voice_preset: è¯­éŸ³é¢„è®¾
            max_chars: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°
            target_tokens: ç›®æ ‡tokenæ•°ï¼ˆ4ä¸‡ï¼‰
            resume: æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        """
        self.voice_preset = voice_preset
        self.max_chars = max_chars
        self.target_tokens = target_tokens
        self.resume = resume
        self.max_workers = max_workers
        
        self.text_processor = TextProcessor(max_chars=max_chars)
        self.audio_generator = EfficientAudioGenerator(
            voice_preset=voice_preset, 
            max_workers=max_workers,
            enable_memory_optimization=True
        )
        
        # çŠ¶æ€æ–‡ä»¶è·¯å¾„
        self.state_file = "token_batch_processing_state.json"
        self.output_dir = "token_batch_audio_files"
        
    def save_state(self, state: dict):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    
    def load_state(self) -> dict:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        ä¸­æ–‡ï¼š1ä¸ªå­—ç¬¦ â‰ˆ 1ä¸ªtoken
        è‹±æ–‡ï¼š1ä¸ªå•è¯ â‰ˆ 1.3ä¸ªtoken
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•° * 1.3
        import re
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        # ç»Ÿè®¡è‹±æ–‡å•è¯
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        
        # ä¼°ç®—tokenæ•°
        estimated_tokens = chinese_chars + int(english_words * 1.3)
        
        return estimated_tokens
    
    def create_token_batches(self, structure_data: dict) -> List[dict]:
        """
        æŒ‰4ä¸‡tokenåˆ›å»ºæ‰¹æ¬¡ï¼Œä¿è¯é¡µé¢æˆ–æ®µè½å®Œæ•´æ€§
        
        Args:
            structure_data: PDFç»“æ„æ•°æ®
            
        Returns:
            æ‰¹æ¬¡åˆ—è¡¨
        """
        pages = structure_data['pages']
        has_page_numbers = structure_data['has_page_numbers']
        
        batches = []
        current_batch = {
            'pages': [],
            'paragraphs': [],
            'text': '',
            'token_count': 0,
            'char_count': 0,
            'start_page': None,
            'end_page': None,
            'start_para': None,
            'end_para': None
        }
        
        print(f"\nå¼€å§‹åˆ›å»º4ä¸‡tokenæ‰¹æ¬¡...")
        print(f"ç›®æ ‡tokenæ•°: {self.target_tokens:,}")
        print(f"é¡µç æ£€æµ‹: {'æœ‰' if has_page_numbers else 'æ— '}")
        
        for page_idx, page_data in enumerate(pages):
            page_text = page_data['text']
            page_tokens = self.estimate_tokens(page_text)
            page_chars = len(page_text)
            
            # å¦‚æœæ·»åŠ å½“å‰é¡µä¼šè¶…è¿‡ç›®æ ‡tokenæ•°ï¼Œä¸”å½“å‰æ‰¹æ¬¡ä¸ä¸ºç©º
            if current_batch['token_count'] + page_tokens > self.target_tokens and current_batch['pages']:
                # å®Œæˆå½“å‰æ‰¹æ¬¡
                current_batch['end_page'] = current_batch['pages'][-1]['page_num']
                batches.append(current_batch.copy())
                
                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch = {
                    'pages': [page_data],
                    'paragraphs': page_data['paragraphs'].copy(),
                    'text': page_text,
                    'token_count': page_tokens,
                    'char_count': page_chars,
                    'start_page': page_data['page_num'],
                    'end_page': None,
                    'start_para': 0,
                    'end_para': len(page_data['paragraphs']) - 1
                }
            else:
                # æ·»åŠ å½“å‰é¡µåˆ°æ‰¹æ¬¡
                if not current_batch['pages']:
                    current_batch['start_page'] = page_data['page_num']
                    current_batch['start_para'] = 0
                
                current_batch['pages'].append(page_data)
                current_batch['paragraphs'].extend(page_data['paragraphs'])
                current_batch['text'] += '\n\n' + page_text if current_batch['text'] else page_text
                current_batch['token_count'] += page_tokens
                current_batch['char_count'] += page_chars
                current_batch['end_page'] = page_data['page_num']
                current_batch['end_para'] = len(current_batch['paragraphs']) - 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch['pages']:
            batches.append(current_batch)
        
        print(f"âœ“ 4ä¸‡tokenæ‰¹æ¬¡åˆ›å»ºå®Œæˆï¼Œå…± {len(batches)} æ‰¹")
        
        # æ‰“å°æ‰¹æ¬¡ä¿¡æ¯
        for i, batch in enumerate(batches, 1):
            if has_page_numbers:
                print(f"æ‰¹æ¬¡ {i}: ç¬¬{batch['start_page']}-{batch['end_page']}é¡µ, "
                      f"{batch['token_count']:,}tokens, {batch['char_count']:,}å­—ç¬¦")
            else:
                print(f"æ‰¹æ¬¡ {i}: ç¬¬{batch['start_page']}-{batch['end_page']}é¡µ, "
                      f"{len(batch['paragraphs'])}ä¸ªæ®µè½, "
                      f"{batch['token_count']:,}tokens, {batch['char_count']:,}å­—ç¬¦")
        
        return batches
    
    def create_audiobook_token_batch(self, pdf_path: str, output_dir: str = "token_batch_audio_files",
                                   keep_chunks: bool = False) -> List[str]:
        """
        æŒ‰4ä¸‡tokenåˆ†æ‰¹åˆ›å»ºæœ‰å£°è¯»ç‰©ï¼Œæ¯æ‰¹ç”Ÿæˆä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            keep_chunks: æ˜¯å¦ä¿ç•™ä¸­é—´æ–‡ä»¶
            
        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print("=" * 60)
        print(f"ğŸ“š å¼€å§‹æŒ‰4ä¸‡tokenåˆ†æ‰¹åˆ¶ä½œæœ‰å£°è¯»ç‰©ï¼ˆæ¯æ‰¹ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼‰")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        state = self.load_state() if self.resume else {}
        
        # æ­¥éª¤1: æ™ºèƒ½æå–PDFæ–‡æœ¬ï¼ˆå¦‚æœæœªå®Œæˆï¼‰
        if 'batches' not in state:
            print("\næ­¥éª¤ 1/3: æ™ºèƒ½æå–PDFæ–‡æœ¬")
            print("-" * 60)
            
            extractor = SmartPDFExtractor(pdf_path)
            structure_data = extractor.extract_text_with_structure()
            batches = self.create_token_batches(structure_data)
            
            # ä¿å­˜çŠ¶æ€
            state['batches'] = batches
            state['total_batches'] = len(batches)
            state['processed_batches'] = 0
            state['completed_batches'] = []
            state['start_time'] = datetime.now().isoformat()
            state['has_page_numbers'] = structure_data['has_page_numbers']
            self.save_state(state)
            
            print(f"âœ“ 4ä¸‡tokenæ‰¹æ¬¡åˆ›å»ºå®Œæˆï¼Œå…± {len(batches)} æ‰¹")
            print(f"âœ“ é¡µç æ£€æµ‹: {'æœ‰' if structure_data['has_page_numbers'] else 'æ— '}")
            print(f"âœ“ å¹³å‡æ¯æ‰¹: {sum(batch['token_count'] for batch in batches) / len(batches):.0f} tokens")
        else:
            batches = state['batches']
            print(f"\nâœ“ æ¢å¤å¤„ç†ï¼Œå…± {len(batches)} æ‰¹")
            print(f"âœ“ å·²å®Œæˆ {state['processed_batches']} æ‰¹")
        
        # æ­¥éª¤2: æŒ‰æ‰¹æ¬¡ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        print("\næ­¥éª¤ 2/3: æŒ‰æ‰¹æ¬¡ç”ŸæˆéŸ³é¢‘æ–‡ä»¶")
        print("-" * 60)
        
        os.makedirs(output_dir, exist_ok=True)
        total_batches = len(batches)
        processed_batches = state.get('processed_batches', 0)
        
        print(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        print(f"å¹¶è¡Œè¿›ç¨‹: {self.max_workers}ä¸ª")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        audio_files = []
        
        for batch_idx in range(processed_batches, total_batches):
            batch = batches[batch_idx]
            
            print(f"\n--- å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ---")
            if state.get('has_page_numbers', False):
                print(f"é¡µé¢èŒƒå›´: ç¬¬{batch['start_page']}-{batch['end_page']}é¡µ")
            else:
                print(f"é¡µé¢èŒƒå›´: ç¬¬{batch['start_page']}-{batch['end_page']}é¡µ")
                print(f"æ®µè½æ•°: {len(batch['paragraphs'])}ä¸ªæ®µè½")
            print(f"Tokenæ•°: {batch['token_count']:,}tokens")
            print(f"å­—ç¬¦æ•°: {batch['char_count']:,}å­—ç¬¦")
            
            # å°†æ‰¹æ¬¡æ–‡æœ¬åˆ†å‰²æˆç‰‡æ®µ
            batch_chunks = self.text_processor.split_into_chunks(batch['text'])
            print(f"ç‰‡æ®µæ•°: {len(batch_chunks)}ä¸ªç‰‡æ®µ")
            
            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„éŸ³é¢‘
            batch_start_time = time.time()
            
            # ä½¿ç”¨é«˜æ•ˆå¹¶è¡Œå¤„ç†
            batch_audio_files = self.audio_generator.generate_audio_batch_efficient(
                batch_chunks, 
                os.path.join(output_dir, f"batch_{batch_idx + 1:03d}_chunks"), 
                batch_size=15
            )
            
            # åˆå¹¶å½“å‰æ‰¹æ¬¡çš„éŸ³é¢‘
            batch_output_path = os.path.join(output_dir, f"batch_{batch_idx + 1:03d}.wav")
            merged_audio = self.audio_generator.merge_audio_files(
                batch_audio_files, 
                batch_output_path,
                silence_duration=0.2
            )
            
            batch_time = time.time() - batch_start_time
            print(f"âœ“ æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆï¼Œè€—æ—¶: {batch_time/60:.1f} åˆ†é’Ÿ")
            print(f"âœ“ å¹³å‡æ¯ç‰‡æ®µ: {batch_time/len(batch_chunks):.2f} ç§’")
            print(f"âœ“ å¤„ç†é€Ÿåº¦: {len(batch_chunks)/batch_time:.1f} it/s")
            print(f"âœ“ è¾“å‡ºæ–‡ä»¶: {batch_output_path}")
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            if merged_audio:
                audio_files.append(batch_output_path)
            
            # æ›´æ–°çŠ¶æ€
            state['processed_batches'] = batch_idx + 1
            state['completed_batches'].append(batch_idx)
            state['last_update'] = datetime.now().isoformat()
            self.save_state(state)
            
            # æ¸…ç†ä¸´æ—¶ç‰‡æ®µæ–‡ä»¶
            if not keep_chunks and batch_audio_files:
                import shutil
                temp_dir = os.path.join(output_dir, f"batch_{batch_idx + 1:03d}_chunks")
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_idx < total_batches - 1:
                print("æ‰¹æ¬¡é—´ä¼‘æ¯ 3 ç§’...")
                time.sleep(3)
        
        # æ­¥éª¤3: ç”Ÿæˆæ‰¹æ¬¡åˆ—è¡¨æ–‡ä»¶
        print("\næ­¥éª¤ 3/3: ç”Ÿæˆæ‰¹æ¬¡åˆ—è¡¨æ–‡ä»¶")
        print("-" * 60)
        
        # ç”Ÿæˆæ‰¹æ¬¡ä¿¡æ¯æ–‡ä»¶
        batch_info = {
            'total_batches': total_batches,
            'audio_files': audio_files,
            'batches': []
        }
        
        for i, batch in enumerate(batches):
            batch_info['batches'].append({
                'batch_number': i + 1,
                'audio_file': audio_files[i] if i < len(audio_files) else None,
                'page_range': f"{batch['start_page']}-{batch['end_page']}",
                'token_count': batch['token_count'],
                'char_count': batch['char_count'],
                'paragraph_count': len(batch['paragraphs'])
            })
        
        # ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯
        info_file = os.path.join(output_dir, "batch_info.json")
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ’­æ”¾åˆ—è¡¨æ–‡ä»¶
        playlist_file = os.path.join(output_dir, "playlist.m3u")
        with open(playlist_file, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            for i, audio_file in enumerate(audio_files, 1):
                f.write(f"#EXTINF:-1,æ‰¹æ¬¡ {i}\n")
                f.write(f"{os.path.basename(audio_file)}\n")
        
        # æ¸…ç†çŠ¶æ€æ–‡ä»¶
        if not keep_chunks and os.path.exists(self.state_file):
            os.remove(self.state_file)
        
        # è®¡ç®—æ€»è€—æ—¶
        if 'start_time' in state:
            start_time = datetime.fromisoformat(state['start_time'])
            total_time = datetime.now() - start_time
            total_tokens = sum(batch['token_count'] for batch in batches)
            total_chunks = sum(len(self.text_processor.split_into_chunks(batch['text'])) for batch in batches)
            
            print(f"\nâœ“ æ€»è€—æ—¶: {total_time.total_seconds()/3600:.1f} å°æ—¶")
            print(f"âœ“ æ€»Tokenæ•°: {total_tokens:,}tokens")
            print(f"âœ“ æ€»ç‰‡æ®µæ•°: {total_chunks:,}ä¸ªç‰‡æ®µ")
            print(f"âœ“ å¹³å‡æ¯ç‰‡æ®µ: {total_time.total_seconds()/total_chunks:.2f} ç§’")
            print(f"âœ“ æ€»å¤„ç†é€Ÿåº¦: {total_chunks/total_time.total_seconds():.1f} it/s")
            print(f"âœ“ é€Ÿåº¦æå‡: çº¦ {total_chunks/total_time.total_seconds()/50:.1f}å€")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“š æŒ‰4ä¸‡tokenåˆ†æ‰¹æœ‰å£°è¯»ç‰©åˆ¶ä½œå®Œæˆï¼")
        print(f"è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
        print(f"éŸ³é¢‘æ–‡ä»¶: {len(audio_files)}ä¸ª")
        print(f"æ‰¹æ¬¡ä¿¡æ¯: {info_file}")
        print(f"æ’­æ”¾åˆ—è¡¨: {playlist_file}")
        print("=" * 60)
        
        return audio_files


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="æŒ‰4ä¸‡tokenåˆ†æ‰¹PDFå°è¯´è½¬æœ‰å£°è¯»ç‰©å·¥å…·ï¼ˆæ¯æ‰¹ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼‰")
    parser.add_argument("pdf_file", help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output-dir", default="token_batch_audio_files", 
                       help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: token_batch_audio_filesï¼‰")
    parser.add_argument("-v", "--voice", default="v2/zh_speaker_1", 
                       help="è¯­éŸ³é¢„è®¾ï¼ˆé»˜è®¤: v2/zh_speaker_1ï¼‰")
    parser.add_argument("-c", "--max-chars", type=int, default=200,
                       help="æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤: 200ï¼‰")
    parser.add_argument("-t", "--target-tokens", type=int, default=40000,
                       help="ç›®æ ‡tokenæ•°ï¼ˆé»˜è®¤: 40000ï¼‰")
    parser.add_argument("-w", "--workers", type=int, default=6,
                       help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 6ï¼‰")
    parser.add_argument("--keep-chunks", action="store_true",
                       help="ä¿ç•™éŸ³é¢‘ç‰‡æ®µæ–‡ä»¶")
    parser.add_argument("--no-resume", action="store_true",
                       help="ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.pdf_file):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_file}")
        return
    
    # åˆ›å»ºæŒ‰tokenåˆ†æ‰¹å¤„ç†åˆ¶ä½œå™¨
    maker = TokenBatchAudiobookMaker(
        voice_preset=args.voice,
        max_chars=args.max_chars,
        target_tokens=args.target_tokens,
        resume=not args.no_resume,
        max_workers=args.workers
    )
    
    maker.create_audiobook_token_batch(
        pdf_path=args.pdf_file,
        output_dir=args.output_dir,
        keep_chunks=args.keep_chunks
    )


if __name__ == "__main__":
    main()
